%% Bayesian intro
\lecture{Bayesian Inference}

\begin{frame}
	\frametitle{Bayesian Inference}
	Standard setup:
	\begin{itemize}
		\item We have some data $\bX = \{\bx_1,\ldots,\bx_N\}$
		\item We have a model $p(\bX|\bPars)$
		\item We define a prior $p(\bPars)$
		\visible<2->{
			\item We use Bayes rule (and typically lots of computation) to compute (or estimate) the posterior:
			\[
				p(\bPars|\bX) = \frac{p(\bX|\bPars)p(\bPars)}{p(\bX)}
			\]
		}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Why Be Bayesian?}
	\begin{itemize}
		\item<2->Ability to incoroporate prior information?
		\item<3->Ability to compute posterior densities (combine prior with likelihood)?
		\item<4->Ability to compare models via marginal likelihood?
		\item<5->For me: the ability to integrate out model parameters completely\ldots
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Why be Bayesian?}
	\begin{itemize}
		\item We're often not interested in parameter values
		\item We're normally interested in something that is a function of the parameter values e.g.:
		\begin{itemize}
			\item Within \ac{ML} we are often interested in making predictions (predicing $y_*$ from $\bx_*$).
			\item This will often require values of some parameters $\bPars$
			\item Being Bayesian allows us to \emph{average} over uncertainity in parameters when making predictions:
			\[
				p(y_*|\bx_*,\bX) = \int p(y_*|\bx_*,\bPars)p(\bPars|\bX)~d\bPars
			\]		
		\end{itemize}
		\item This for me, is the biggest Bayesian selling point!
	\end{itemize}
\end{frame}